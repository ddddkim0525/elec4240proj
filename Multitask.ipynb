{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from dataset import train_dataloader, test_dataloader\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "nc = 3\n",
    "lr = 1\n",
    "beta1 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(32,3,32)\n",
    "y = torch.rand(32,3,32)\n",
    "\n",
    "z = torch.cat((x,y),0)\n",
    "z = torch.mean(z,axis=-1)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilossD(pred_concat, target_concat):\n",
    "    mean = torch.mean(torch.square(pred_concat - target_concat), axis = 0)\n",
    "    return torch.exp(torch.mean(torch.log(mean), axis = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilossG(pred_nor,pred_disp,pred_rough,data):\n",
    "    mse_nor_mean = torch.mean(torch.square(pred_nor - data[\"nor\"].to(device)), axis = 0)\n",
    "    mse_disp_mean = torch.mean(torch.square(pred_disp - data[\"disp\"].to(device)), axis = 0)\n",
    "    mse_rough_mean = torch.mean(torch.square(pred_rough - data[\"rough\"].to(device)), axis = 0)\n",
    "    mse_concat = torch.cat((mse_nor_mean, mse_disp_mean, mse_rough_mean),0)\n",
    "    mse_concat = torch.mean(torch.log(mse_concat),axis = -1)\n",
    "    mse_concat = torch.mean(mse_concat,axis = -1)\n",
    "    return torch.exp(torch.mean(mse_concat,axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1601, 0.1741, 0.1610])\n"
     ]
    }
   ],
   "source": [
    "print(multilossD(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCGAN - Disp\n",
    "from network import GeneratorSkipMultitask, Discriminator,BasicBlock, weights_init\n",
    "\n",
    "netG = GeneratorSkipMultitask(ngf,BasicBlock).to(device)\n",
    "netG.apply(weights_init)\n",
    "netD_nor = Discriminator(ndf,3).to(device)\n",
    "netD_disp = Discriminator(ndf,1).to(device)\n",
    "netD_rough = Discriminator(ndf,1).to(device)\n",
    "netD_nor.apply(weights_init)\n",
    "netD_disp.apply(weights_init)\n",
    "netD_rough.apply(weights_init)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "mse = nn.MSELoss()\n",
    "def lr_schedule(epoch):\n",
    "    lr = 0\n",
    "    if(epoch < 8):\n",
    "        lr = 0.001 * ((epoch+1)/8)\n",
    "    else:\n",
    "        lr = 0.001*(8/epoch)\n",
    "    return lr\n",
    "\n",
    "optimizerD_nor = optim.Adam(netD_nor.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "DScheduler_nor = torch.optim.lr_scheduler.LambdaLR(optimizerD_nor, lr_schedule)\n",
    "optimizerD_disp = optim.Adam(netD_disp.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "DScheduler_disp = torch.optim.lr_scheduler.LambdaLR(optimizerD_disp, lr_schedule)\n",
    "optimizerD_rough = optim.Adam(netD_rough.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "DScheduler_rough = torch.optim.lr_scheduler.LambdaLR(optimizerD_rough, lr_schedule)\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "GScheduler = torch.optim.lr_scheduler.LambdaLR(optimizerG, lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "0 errD:  5.751616954803467  errG:  0.4495849609375\n",
      "TEST MSE_nor:  tensor(0.0499)\n",
      "TEST MSE_disp:  tensor(0.0673)\n",
      "TEST MSE_rough:  tensor(0.1205)\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "real_label = 1.0\n",
    "fake_label = 0.0\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "test_MSE_nor = []\n",
    "test_MSE_disp = []\n",
    "test_MSE_rough = []\n",
    "iters = 0\n",
    "num_epochs = 30\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        netD_nor.zero_grad()\n",
    "        netD_disp.zero_grad()\n",
    "        netD_rough.zero_grad()\n",
    "        # Format batch\n",
    "        real_nor = data[\"nor\"].to(device)\n",
    "        real_disp = data[\"disp\"].to(device)\n",
    "        real_rough = data[\"rough\"].to(device)\n",
    "        b_size = real_nor.size(0)\n",
    "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
    "        # Forward pass real batch through D\n",
    "        output_nor = netD_nor(real_nor).view(-1)\n",
    "        output_disp = netD_disp(real_disp).view(-1)\n",
    "        output_rough = netD_rough(real_rough).view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real_nor = criterion(output_nor, label)\n",
    "        errD_real_disp = criterion(output_disp, label)\n",
    "        errD_real_rough = criterion(output_rough, label)\n",
    "        errD_real = errD_real_nor + errD_real_disp + errD_real_rough\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real_nor.backward()\n",
    "        errD_real_disp.backward()\n",
    "        errD_real_rough.backward()\n",
    "        D_x_nor = output_nor.mean().item()\n",
    "        D_x_disp = output_disp.mean().item()\n",
    "        D_x_rough = output_rough.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = data[\"diff\"].to(device)\n",
    "        # Generate fake image batch with G\n",
    "        fake_nor, fake_disp, fake_rough = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output_nor = netD_nor(fake_nor.detach()).view(-1)\n",
    "        output_disp = netD_disp(fake_disp.detach()).view(-1)\n",
    "        output_rough = netD_rough(fake_rough.detach()).view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake_nor = criterion(output_nor, label)\n",
    "        errD_fake_disp = criterion(output_disp, label)\n",
    "        errD_fake_rough = criterion(output_rough, label)\n",
    "        errD_fake = errD_fake_nor + errD_fake_disp + errD_fake_rough\n",
    "        # Calculate the gradients for this batch\n",
    "        errD_fake_nor.backward()\n",
    "        errD_fake_disp.backward()\n",
    "        errD_fake_rough.backward()\n",
    "        D_G_z1_nor = output_nor.mean().item()\n",
    "        D_G_z1_disp = output_disp.mean().item()\n",
    "        D_G_z1_rough = output_rough.mean().item()\n",
    "        # Add the gradients from the all-real and all-fake batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD_nor.step()\n",
    "        optimizerD_disp.step()\n",
    "        optimizerD_rough.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output_nor = netD_nor(fake_nor).view(-1)\n",
    "        output_disp = netD_disp(fake_disp).view(-1)\n",
    "        output_rough = netD_rough(fake_rough).view(-1)  \n",
    "        output = torch.cat((output_nor, output_disp, output_rough),0)\n",
    "        # Calculate G's loss based on this output\n",
    "        errG = (multilossG(fake_nor,fake_disp,fake_rough, data)  + multilossD(output,torch.cat((label,label,label),0)))/2\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "        loss_fn = nn.MSELoss()\n",
    "        data_test = next(iter(test_dataloader))\n",
    "        fake_nor, fake_disp, fake_rough = netG(data_test[\"diff\"].to(device))\n",
    "        test_MSE_nor.append(mse(fake_nor.detach().cpu(),data_test[\"nor\"]))\n",
    "        test_MSE_disp.append(mse(fake_disp.detach().cpu(),data_test[\"disp\"]))\n",
    "        test_MSE_rough.append(mse(fake_rough.detach().cpu(),data_test[\"rough\"]))\n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print(epoch,\"errD: \",errD.item(), \" errG: \",errG.item())\n",
    "#             print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tMSE_LossG: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "#                     % (epoch, num_epochs, i, len(train_dataloader),\n",
    "#                         errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "            print(\"TEST MSE_nor: \", test_MSE_nor[-1])\n",
    "            print(\"TEST MSE_disp: \", test_MSE_disp[-1])\n",
    "            print(\"TEST MSE_rough: \", test_MSE_rough[-1])\n",
    "\n",
    "\n",
    "        \n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "#         if iters % 10 == 0:\n",
    "#             with torch.no_grad():\n",
    "#                 loss_fn = nn.MSELoss()\n",
    "#                 data = next(iter(test_dataloader))\n",
    "#                 fake = netG(data[\"diff\"].to(device)).detach().cpu()\n",
    "#                 test_MSE_losses.append(mse(fake,data[\"nor\"]))\n",
    "#                 print(\"TEST MSE: \", mse(fake,data[\"nor\"]))\n",
    "#             img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "#         iters += 1\n",
    "    DScheduler_nor.step()\n",
    "    DScheduler_disp.step()\n",
    "    DScheduler_rough.step()\n",
    "    GScheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "errG = (multilossG(fake_nor,fake_disp,fake_rough, data)  + multilossD(output,torch.cat((label,label,label),0)))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2233e-07, device='cuda:0', grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilossD(output,torch.cat((label,label,label),0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Multitask MSE Loss\")\n",
    "plt.plot(test_MSE_nor,label=\"nor_MSE\")\n",
    "plt.plot(test_MSE_disp,label=\"disp_MSE\")\n",
    "plt.plot(test_MSE_rough,label=\"rough_MSE\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
